{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-07T15:48:09.067103Z","iopub.execute_input":"2025-11-07T15:48:09.067379Z","iopub.status.idle":"2025-11-07T15:48:10.540629Z","shell.execute_reply.started":"2025-11-07T15:48:09.067353Z","shell.execute_reply":"2025-11-07T15:48:10.539809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton unsloth\n!pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n!pip install --quiet huggingface_hub\n!pip install unsloth_zoo","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T16:00:36.683622Z","iopub.execute_input":"2025-11-07T16:00:36.684027Z","iopub.status.idle":"2025-11-07T16:00:45.280616Z","shell.execute_reply.started":"2025-11-07T16:00:36.683987Z","shell.execute_reply":"2025-11-07T16:00:45.279598Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"1Ô∏è‚É£ Load LoRA fine-tuned model","metadata":{}},{"cell_type":"code","source":"import os\nfrom datetime import datetime\nfrom datasets import load_dataset\nfrom huggingface_hub import HfApi, upload_folder\nfrom unsloth import FastModel\nfrom unsloth.chat_templates import get_chat_template, train_on_responses_only\nfrom trl import SFTTrainer, SFTConfig","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T16:00:49.960014Z","iopub.execute_input":"2025-11-07T16:00:49.960317Z","iopub.status.idle":"2025-11-07T16:01:28.078120Z","shell.execute_reply.started":"2025-11-07T16:00:49.960288Z","shell.execute_reply":"2025-11-07T16:01:28.077528Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport sys\n\nif not torch.cuda.is_available():\n    print(\"‚ùå No GPU available ‚Äî exiting notebook\")\n    sys.exit(1)  # Exit with failure\nelse:\n    device = torch.device(\"cuda\")\n    print(f\"‚úÖ GPU detected: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n\nHF_REPO_ID = \"HamzaKhan-03/Hamza_the_doctor.ai\"\n\nprint(f\"‚úÖ Using Hugging Face repo: {HF_REPO_ID}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T16:02:12.060239Z","iopub.execute_input":"2025-11-07T16:02:12.060817Z","iopub.status.idle":"2025-11-07T16:02:12.199370Z","shell.execute_reply.started":"2025-11-07T16:02:12.060794Z","shell.execute_reply":"2025-11-07T16:02:12.198743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"‚¨áÔ∏è Loading model + tokenizer from Hugging Face...\")\nmodel, tokenizer = FastModel.from_pretrained(\n    HF_REPO_ID,\n    load_in_4bit=True,   # Efficient low-bit training\n    token=HF_TOKEN,\n)\nmodel.to(\"cuda\")\nprint(\"‚úÖ Model loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T16:02:18.763376Z","iopub.execute_input":"2025-11-07T16:02:18.764130Z","iopub.status.idle":"2025-11-07T16:02:41.457886Z","shell.execute_reply.started":"2025-11-07T16:02:18.764106Z","shell.execute_reply":"2025-11-07T16:02:41.456948Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2Ô∏è‚É£ Prepare Dataset","metadata":{}},{"cell_type":"code","source":"print(\"‚¨áÔ∏è Loading dataset (lavita/ChatDoctor-HealthCareMagic-100k)...\")\ndataset = load_dataset(\"lavita/ChatDoctor-HealthCareMagic-100k\", split=\"train\")\n\ndef convert_to_chatml(example):\n    return {\n        \"conversations\": [\n            {\"role\": \"system\", \"content\": example[\"instruction\"]},\n            {\"role\": \"user\", \"content\": example[\"input\"]},\n            {\"role\": \"assistant\", \"content\": example[\"output\"]},\n        ]\n    }\n\ndataset = dataset.map(convert_to_chatml)\nprint(\"‚úÖ Converted dataset to chatML format\")\n\n# Apply Gemma chat template\ntokenizer = get_chat_template(tokenizer, chat_template=\"gemma3\")\n\ndef formatting_prompts_func(examples):\n    convos = examples[\"conversations\"]\n    texts = [\n        tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False).removeprefix(\"<bos>\")\n        for convo in convos\n    ]\n    return {\"text\": texts}\n\ndataset = dataset.map(formatting_prompts_func, batched=True)\nprint(\"‚úÖ Applied chat template\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T16:02:48.861140Z","iopub.execute_input":"2025-11-07T16:02:48.861772Z","iopub.status.idle":"2025-11-07T16:03:09.519311Z","shell.execute_reply.started":"2025-11-07T16:02:48.861748Z","shell.execute_reply":"2025-11-07T16:03:09.518566Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3Ô∏è‚É£ Fine-tuning Configuration","metadata":{}},{"cell_type":"code","source":"print(\"‚öôÔ∏è Setting up trainer for 1 epoch run...\")\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=dataset.shuffle(seed=42).select(range(2000)),  # subset for quick daily run\n    eval_dataset=None,\n    args=SFTConfig(\n        dataset_text_field=\"text\",\n        per_device_train_batch_size=8,\n        gradient_accumulation_steps=1,\n        warmup_steps=5,\n        num_train_epochs=1,\n        learning_rate=5e-5,\n        logging_steps=10,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"/kaggle/working/my_lora_model\",\n        report_to=\"none\",\n        save_strategy=\"steps\",\n        save_steps=1000,\n        save_total_limit=1,\n        load_best_model_at_end=False,\n    ),\n)\n\ntrainer = train_on_responses_only(\n    trainer,\n    instruction_part=\"<start_of_turn>user\\n\",\n    response_part=\"<start_of_turn>model\\n\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T16:03:32.970089Z","iopub.execute_input":"2025-11-07T16:03:32.970756Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"4Ô∏è‚É£ Train and Save Model as .safetensors","metadata":{}},{"cell_type":"code","source":"print(\"üöÄ Starting fine-tuning (1 epoch)...\")\ntrainer.train()\nprint(\"‚úÖ Training complete!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SAVE_DIR = \"/kaggle/working/my_lora_model\"\nprint(f\"üíæ Saving merged model to {SAVE_DIR}...\")\n\nFastModel.save_pretrained(\n    model,\n    save_directory=SAVE_DIR,\n    tokenizer=tokenizer,\n    merge_lora=True,\n    save_safetensors=True,\n)\nprint(\"‚úÖ Model saved successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"5Ô∏è‚É£ Upload to Kaggle Dataset (for GitHub sync)","metadata":{}},{"cell_type":"code","source":"print(\"‚¨ÜÔ∏è Uploading updated weights to Hugging Face...\")\n\napi = HfApi()\ncommit_message = f\"Daily fine-tune update ‚Äî {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}\"\n\napi.upload_folder(\n    repo_id=HF_REPO_ID,\n    folder_path=SAVE_DIR,\n    commit_message=commit_message,\n    token=HF_TOKEN,\n)\nprint(f\"‚úÖ Successfully pushed new version: https://huggingface.co/{HF_REPO_ID}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nshutil.rmtree(SAVE_DIR, ignore_errors=True)\nprint(\"üßπ Cleaned temporary model files. All done!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}